{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "sys.path.append('thirdparty/ABAW2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.remove('/mnt/DATA2/congvm/Workspace/ABAW2')\n",
    "from utils.datamodule import get_transform, AffWildDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Get ImageNet transform funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 253/253 [00:07<00:00, 35.28it/s] \n",
      "100%|██████████| 70/70 [00:00<00:00, 144.85it/s]\n"
     ]
    }
   ],
   "source": [
    "data_module = AffWildDataModule(\n",
    "    data_dir=\"/mnt/DATA1/hung/ABAW/data\",\n",
    "    backbone_name=\"vggresnet50\",\n",
    "    mode=\"static\",\n",
    "    batch_size=1,\n",
    ")\n",
    "data_module.setup(stage=\"fit\")\n",
    "val_dataloader = data_module.val_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 112, 112])\n",
      "['/mnt/DATA1/hung/ABAW/data/cropped_aligned/6-30-1920x1080_right/00001.jpg']\n"
     ]
    }
   ],
   "source": [
    "for batch in val_dataloader:\n",
    "    break\n",
    "print(batch['img_arr'].shape)\n",
    "print(batch['img_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check transform\n",
    "from torchvision import transforms\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(size=(112, 112)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr = cv2.imread(batch['img_path'][0])[..., ::-1]\n",
    "img_arr = test_transform(img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.5356624 , -1.6041614 , -1.5870366 , ..., -1.2787911 ,\n",
       "         -1.2102921 , -1.1589178 ],\n",
       "        [-1.5699118 , -1.6384109 , -1.6555357 , ..., -1.3301654 ,\n",
       "         -1.2616663 , -1.2787911 ],\n",
       "        [-1.6212862 , -1.6555357 , -1.6726604 , ..., -1.3986644 ,\n",
       "         -1.4157891 , -1.4500387 ],\n",
       "        ...,\n",
       "        [-1.3815396 , -1.3815396 , -1.3815396 , ..., -1.6897851 ,\n",
       "         -1.6384109 , -1.5870366 ],\n",
       "        [-1.4157891 , -1.3986644 , -1.3986644 , ..., -1.7240347 ,\n",
       "         -1.6555357 , -1.5699118 ],\n",
       "        [-1.4329139 , -1.4329139 , -1.4157891 , ..., -1.7411594 ,\n",
       "         -1.6555357 , -1.5527872 ]],\n",
       "\n",
       "       [[-1.5280112 , -1.5630252 , -1.5630252 , ..., -1.317927  ,\n",
       "         -1.247899  , -1.1953781 ],\n",
       "        [-1.5630252 , -1.6155462 , -1.5805322 , ..., -1.317927  ,\n",
       "         -1.30042   , -1.265406  ],\n",
       "        [-1.5980392 , -1.6330532 , -1.5980392 , ..., -1.370448  ,\n",
       "         -1.405462  , -1.4229691 ],\n",
       "        ...,\n",
       "        [-0.93277305, -0.93277305, -0.93277305, ..., -1.352941  ,\n",
       "         -1.3354341 , -1.2829131 ],\n",
       "        [-0.9502801 , -0.9502801 , -0.9502801 , ..., -1.3879551 ,\n",
       "         -1.317927  , -1.230392  ],\n",
       "        [-0.9677871 , -0.9502801 , -0.9677871 , ..., -1.405462  ,\n",
       "         -1.317927  , -1.212885  ]],\n",
       "\n",
       "       [[-1.3687146 , -1.4210021 , -1.3512855 , ..., -1.1247058 ,\n",
       "         -1.0549891 , -1.0027015 ],\n",
       "        [-1.403573  , -1.4384314 , -1.3861438 , ..., -1.1421349 ,\n",
       "         -1.1072767 , -1.0898474 ],\n",
       "        [-1.4210021 , -1.4558606 , -1.403573  , ..., -1.1944225 ,\n",
       "         -1.229281  , -1.2467101 ],\n",
       "        ...,\n",
       "        [ 0.39163405,  0.32191727,  0.18248373, ..., -0.44496724,\n",
       "         -0.44496724, -0.39267966],\n",
       "        [ 0.40906325,  0.3044881 ,  0.16505454, ..., -0.47982562,\n",
       "         -0.44496724, -0.35782126],\n",
       "        [ 0.39163405,  0.32191727,  0.14762534, ..., -0.49725482,\n",
       "         -0.44496724, -0.34039208]]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['img_arr'][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.5356624 , -1.6041614 , -1.5870366 , ..., -1.2787911 ,\n",
       "         -1.2102921 , -1.1589178 ],\n",
       "        [-1.5699118 , -1.6384109 , -1.6555357 , ..., -1.3301654 ,\n",
       "         -1.2616663 , -1.2787911 ],\n",
       "        [-1.6212862 , -1.6555357 , -1.6726604 , ..., -1.3986644 ,\n",
       "         -1.4157891 , -1.4500387 ],\n",
       "        ...,\n",
       "        [-1.3815396 , -1.3815396 , -1.3815396 , ..., -1.6897851 ,\n",
       "         -1.6384109 , -1.5870366 ],\n",
       "        [-1.4157891 , -1.3986644 , -1.3986644 , ..., -1.7240347 ,\n",
       "         -1.6555357 , -1.5699118 ],\n",
       "        [-1.4329139 , -1.4329139 , -1.4157891 , ..., -1.7411594 ,\n",
       "         -1.6555357 , -1.5527872 ]],\n",
       "\n",
       "       [[-1.5280112 , -1.5630252 , -1.5630252 , ..., -1.317927  ,\n",
       "         -1.247899  , -1.1953781 ],\n",
       "        [-1.5630252 , -1.6155462 , -1.5805322 , ..., -1.317927  ,\n",
       "         -1.30042   , -1.265406  ],\n",
       "        [-1.5980392 , -1.6330532 , -1.5980392 , ..., -1.370448  ,\n",
       "         -1.405462  , -1.4229691 ],\n",
       "        ...,\n",
       "        [-0.93277305, -0.93277305, -0.93277305, ..., -1.352941  ,\n",
       "         -1.3354341 , -1.2829131 ],\n",
       "        [-0.9502801 , -0.9502801 , -0.9502801 , ..., -1.3879551 ,\n",
       "         -1.317927  , -1.230392  ],\n",
       "        [-0.9677871 , -0.9502801 , -0.9677871 , ..., -1.405462  ,\n",
       "         -1.317927  , -1.212885  ]],\n",
       "\n",
       "       [[-1.3687146 , -1.4210021 , -1.3512855 , ..., -1.1247058 ,\n",
       "         -1.0549891 , -1.0027015 ],\n",
       "        [-1.403573  , -1.4384314 , -1.3861438 , ..., -1.1421349 ,\n",
       "         -1.1072767 , -1.0898474 ],\n",
       "        [-1.4210021 , -1.4558606 , -1.403573  , ..., -1.1944225 ,\n",
       "         -1.229281  , -1.2467101 ],\n",
       "        ...,\n",
       "        [ 0.39163405,  0.32191727,  0.18248373, ..., -0.44496724,\n",
       "         -0.44496724, -0.39267966],\n",
       "        [ 0.40906325,  0.3044881 ,  0.16505454, ..., -0.47982562,\n",
       "         -0.44496724, -0.35782126],\n",
       "        [ 0.39163405,  0.32191727,  0.14762534, ..., -0.49725482,\n",
       "         -0.44496724, -0.34039208]]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_arr.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(batch['img_arr'][0].numpy() - img_arr.numpy()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(batch['img_arr'][0].numpy(), img_arr.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/mnt/DATA1/hung/ABAW/src/weight/multitask_best_ex_3.pth\"\n",
    "model = torch.load(PATH)\n",
    "_ = model.eval()\n",
    "# state_dict = ckpt.state_dict()\n",
    "# state_dict_ = state_dict.copy()\n",
    "# for k, v in state_dict.items():\n",
    "#     state_dict_[k.replace('module.', '')] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = iter(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8466097] [4]\n",
      "tensor([4])\n"
     ]
    }
   ],
   "source": [
    "batch = next(val_dataloader)\n",
    "y_pred_exp, y_pred_au = model(batch['img_arr'])\n",
    "y_pred_au = y_pred_au.sigmoid()\n",
    "y_pred_au[y_pred_au >= 0.5] = 1.0\n",
    "y_pred_au[y_pred_au < 0.5] = 0.0\n",
    "\n",
    "y_pred_exp = torch.softmax(y_pred_exp, dim=1)\n",
    "y_pred_exp_prob, y_pred_exp = torch.max(y_pred_exp, dim=1)\n",
    "\n",
    "y_pred_exp_prob = y_pred_exp_prob.detach().cpu().numpy()\n",
    "y_pred_exp = y_pred_exp.detach().cpu().numpy()\n",
    "print(y_pred_exp_prob, y_pred_exp)\n",
    "print(batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Get ImageNet transform funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 253/253 [00:06<00:00, 36.44it/s] \n",
      "100%|██████████| 70/70 [00:00<00:00, 143.92it/s]\n"
     ]
    }
   ],
   "source": [
    "data_module = AffWildDataModule(\n",
    "    data_dir=\"/mnt/DATA1/hung/ABAW/data\",\n",
    "    backbone_name=\"vggresnet50\",\n",
    "    mode=\"static\",\n",
    "    batch_size=32,\n",
    ")\n",
    "data_module.setup(stage=\"fit\")\n",
    "val_dataloader = data_module.val_dataloader()\n",
    "\n",
    "_ = model.to(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7481/7481 [06:19<00:00, 19.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# batch = next(val_dataloader)\n",
    "exp_results = []\n",
    "exp_labels = []\n",
    "for batch in tqdm(val_dataloader):\n",
    "    batch[\"img_arr\"] = batch[\"img_arr\"].to('cuda:0')\n",
    "    with torch.no_grad():\n",
    "        y_pred_exp, y_pred_au = model(batch[\"img_arr\"])\n",
    "    y_pred_au = y_pred_au.sigmoid()\n",
    "    y_pred_au[y_pred_au >= 0.5] = 1.0\n",
    "    y_pred_au[y_pred_au < 0.5] = 0.0\n",
    "\n",
    "    y_pred_exp = torch.softmax(y_pred_exp, dim=1)\n",
    "    y_pred_exp_prob, y_pred_exp = torch.max(y_pred_exp, dim=1)\n",
    "\n",
    "    y_pred_exp_prob = y_pred_exp_prob.detach().cpu().numpy()\n",
    "    y_pred_exp = y_pred_exp.detach().cpu().numpy()\n",
    "    exp_results.extend(y_pred_exp)\n",
    "    exp_labels.extend(batch[\"labels\"])\n",
    "    # print(y_pred_exp_prob, y_pred_exp)\n",
    "    # print(batch[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_labels = [e.numpy() for e in exp_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(exp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6503133229480594"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(exp_labels, exp_results, average='macro', labels=range(0, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79de97b11cec26256d2df36864b937bc129bcdd5703af5adf2bc88959b36e7e4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
